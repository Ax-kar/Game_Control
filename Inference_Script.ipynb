{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Neccessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "from ultralytics import YOLO\n",
    "import streamlit as st\n",
    "import webbrowser\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your custom-trained YOLOv8 model\n",
    "\n",
    "model= YOLO(r'C:\\Users\\Administrator\\Desktop\\final_custom_model\\best.pt')\n",
    "\n",
    "# Define gesture-key mappings for controlling the game\n",
    "gesture_key_map = {\n",
    "    'thumbsup': 'up',\n",
    "    'thumbsdown': 'down',\n",
    "    'thumbsleft': 'left',\n",
    "    'thumbsright': 'right',\n",
    "    # Add other mappings as needed\n",
    "}\n",
    "\n",
    "# Define speech command mappings\n",
    "speech_command_map = {\n",
    "    'start': 'x',\n",
    "    'stop': 'y',\n",
    "    \n",
    "    # Add more commands as needed\n",
    "}\n",
    "\n",
    "# Global variable to store detected gestures\n",
    "current_gesture = None\n",
    "\n",
    "# Function to recognize speech in real-time\n",
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "\n",
    "    while True:\n",
    "        with mic as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            print(\"Listening for commands...\")\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower().strip()\n",
    "            print(f\"Recognized command: {command}\")\n",
    "            if command in speech_command_map:\n",
    "                pyautogui.press(speech_command_map[command])\n",
    "                print(f\"Executed command: {command}\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I did not understand that.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "# Function to detect gestures\n",
    "def detect_gestures(frame):\n",
    "    global current_gesture\n",
    "    results = model(frame)\n",
    "    detected_gestures = set()\n",
    "\n",
    "    for result in results:\n",
    "        for bbox in result.boxes:\n",
    "            class_id = int(bbox.cls)\n",
    "            object_class = model.names[class_id]\n",
    "            confidence = bbox.conf\n",
    "\n",
    "            # Check if the confidence level is above a threshold (e.g., 0.5)\n",
    "            if confidence > 0.2:\n",
    "                x1, y1, x2, y2 = map(int, bbox.xyxy[0])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, object_class, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "                if object_class in gesture_key_map:\n",
    "                    detected_gestures.add(object_class)\n",
    "\n",
    "    if detected_gestures:\n",
    "        new_gesture = detected_gestures.pop()\n",
    "        if new_gesture != current_gesture:\n",
    "            current_gesture = new_gesture\n",
    "            print(f\"Detected gesture: {current_gesture}, pressing key: {gesture_key_map[current_gesture]}\")\n",
    "            pyautogui.press(gesture_key_map[current_gesture])\n",
    "    else:\n",
    "        current_gesture = None\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Function to run the game control loop\n",
    "def run_game_control():\n",
    "    global current_gesture\n",
    "\n",
    "    # Open a connection to the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Run the speech recognition in a separate thread\n",
    "    speech_thread = threading.Thread(target=recognize_speech, daemon=True)\n",
    "    speech_thread.start()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = detect_gestures(frame)\n",
    "\n",
    "        # Display the frame with detections\n",
    "        cv2.imshow('Game Control', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Gesture-Controlled Game Player\")\n",
    "st.write(\"Choose a game to play using gestures and voice commands.\")\n",
    "\n",
    "# List of games (Add more links as needed)\n",
    "games = {\n",
    "    \"Tetris\": \"https://tetris.com/play-tetris\",\n",
    "    \"Pac-Man\": \"https://g.co/kgs/9kYA8Cm\",\n",
    "    \"Snake\": \"https://playsnake.org/\",\n",
    "    \"Minesweeper\": \"https://minesweeper.online/\"\n",
    "}\n",
    "\n",
    "game_choice = st.selectbox(\"Select a game\", list(games.keys()))\n",
    "\n",
    "if st.button(\"Start Game\"):\n",
    "    game_url = games[game_choice]\n",
    "    webbrowser.open(game_url)  # Open the selected game in a new browser tab\n",
    "    st.write(f\"Game started: {game_choice}\")\n",
    "\n",
    "    # Run the game control function in a separate thread to keep the Streamlit UI responsive\n",
    "    control_thread = threading.Thread(target=run_game_control, daemon=True)\n",
    "    control_thread.start()\n",
    "\n",
    "# Add a placeholder for displaying the webcam feed (this will be shown in a separate window by OpenCV)\n",
    "st.write(\"Webcam feed will be displayed in a separate window.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
